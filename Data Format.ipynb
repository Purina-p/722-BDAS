{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8cb47f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean, col, when, count, isnan, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5918a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/07 05:28:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n",
    "merged_df = spark.read.csv('Merged_Dataset.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a11612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before imputation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+-----------+------------+----------------+----------+--------------+-----------------------+----------+-----------+-----------+-----------+-----------+----+----+---+---+---+---+------+\n",
      "|iso_code|location|date|total_cases|total_deaths|stringency_index|population|gdp_per_capita|human_development_index|Unnamed: 9|Unnamed: 10|Unnamed: 11|Unnamed: 12|Unnamed: 13|CODE| HDI| TC| TD|STI|POP|GDPCAP|\n",
      "+--------+--------+----+-----------+------------+----------------+----------+--------------+-----------------------+----------+-----------+-----------+-----------+-----------+----+----+---+---+---+---+------+\n",
      "|       0|       0|   0|       3094|       11190|            7126|         0|          5712|                   6202|      3594|      12298|      10042|          0|       5712|   0|6202|  0|  0|  0|  0|     0|\n",
      "+--------+--------+----+-----------+------------+----------------+----------+--------------+-----------------------+----------+-----------+-----------+-----------+-----------+----+----+---+---+---+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 68:>                                                         (0 + 2) / 2]\r",
      "\r",
      "[Stage 68:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+-----------+------------+----------------+----------+--------------+-----------------------+----------+-----------+-----------+-----------+-----------+----+---+---+---+---+---+------+\n",
      "|iso_code|location|date|total_cases|total_deaths|stringency_index|population|gdp_per_capita|human_development_index|Unnamed: 9|Unnamed: 10|Unnamed: 11|Unnamed: 12|Unnamed: 13|CODE|HDI| TC| TD|STI|POP|GDPCAP|\n",
      "+--------+--------+----+-----------+------------+----------------+----------+--------------+-----------------------+----------+-----------+-----------+-----------+-----------+----+---+---+---+---+---+------+\n",
      "|       0|       0|   0|          0|           0|               0|         0|             0|                      0|         0|          0|          0|          0|          0|   0|  0|  0|  0|  0|  0|     0|\n",
      "+--------+--------+----+-----------+------------+----------------+----------+--------------+-----------------------+----------+-----------+-----------+-----------+-----------+----+---+---+---+---+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "missing_values = merged_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in merged_df.columns])\n",
    "print(\"Missing values before imputation:\")\n",
    "missing_values.show()\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    mean_value = merged_df.agg(mean(column)).first()[0]\n",
    "    merged_df = merged_df.withColumn(column, when(col(column).isNull(), lit(mean_value)).otherwise(col(column)))\n",
    "\n",
    "missing_values_after = merged_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in merged_df.columns])\n",
    "print(\"Missing values after imputation:\")\n",
    "missing_values_after.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "956f82a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column total_cases has 9264 outliers before processing.\n",
      "Column total_cases has 0 outliers after processing.\n",
      "Column total_deaths has 2830 outliers before processing.\n",
      "Column total_deaths has 0 outliers after processing.\n",
      "Column stringency_index has 0 outliers before processing.\n",
      "Column stringency_index has 0 outliers after processing.\n",
      "Column population has 5597 outliers before processing.\n",
      "Column population has 0 outliers after processing.\n",
      "Column gdp_per_capita has 2281 outliers before processing.\n",
      "Column gdp_per_capita has 0 outliers after processing.\n",
      "Column human_development_index has 651 outliers before processing.\n",
      "Column human_development_index has 0 outliers after processing.\n",
      "Column Unnamed: 9 has 0 outliers before processing.\n",
      "Column Unnamed: 9 has 0 outliers after processing.\n",
      "Column Unnamed: 10 has 2288 outliers before processing.\n",
      "Column Unnamed: 10 has 0 outliers after processing.\n",
      "Column Unnamed: 11 has 4149 outliers before processing.\n",
      "Column Unnamed: 11 has 0 outliers after processing.\n",
      "Column Unnamed: 12 has 841 outliers before processing.\n",
      "Column Unnamed: 12 has 0 outliers after processing.\n",
      "Column Unnamed: 13 has 420 outliers before processing.\n",
      "Column Unnamed: 13 has 0 outliers after processing.\n",
      "Column HDI has 651 outliers before processing.\n",
      "Column HDI has 0 outliers after processing.\n",
      "Column TC has 0 outliers before processing.\n",
      "Column TC has 0 outliers after processing.\n",
      "Column TD has 0 outliers before processing.\n",
      "Column TD has 0 outliers after processing.\n",
      "Column STI has 10042 outliers before processing.\n",
      "Column STI has 0 outliers after processing.\n",
      "Column POP has 841 outliers before processing.\n",
      "Column POP has 0 outliers after processing.\n",
      "Column GDPCAP has 5712 outliers before processing.\n",
      "Column GDPCAP has 0 outliers after processing.\n"
     ]
    }
   ],
   "source": [
    "for column in [col_name for col_name, dtype in merged_df.dtypes if dtype in [\"double\", \"int\"]]:\n",
    "    quantiles = merged_df.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    median = merged_df.approxQuantile(column, [0.5], 0.01)[0]\n",
    "    \n",
    "    outliers = merged_df.filter((col(column) < (Q1 - 1.5 * IQR)) | (col(column) > (Q3 + 1.5 * IQR)))\n",
    "    print(f\"Column {column} has {outliers.count()} outliers before processing.\")\n",
    "    \n",
    "    merged_df = merged_df.withColumn(column, when((col(column) < (Q1 - 1.5 * IQR)) | (col(column) > (Q3 + 1.5 * IQR)), median).otherwise(col(column)))\n",
    "    \n",
    "    outliers_after = merged_df.filter((col(column) < (Q1 - 1.5 * IQR)) | (col(column) > (Q3 + 1.5 * IQR)))\n",
    "    print(f\"Column {column} has {outliers_after.count()} outliers after processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74841169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the processed dataframe:\n",
      "['iso_code', 'location', 'date', 'total_cases', 'total_deaths', 'stringency_index', 'population', 'gdp_per_capita', 'human_development_index', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'CODE', 'HDI', 'TC', 'TD', 'STI', 'POP', 'GDPCAP']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "column_freq = [col for col, count in merged_df.groupBy(merged_df.columns).count().alias('count').groupby(merged_df.columns).agg(F.sum('count')).toPandas().set_index(merged_df.columns).T.to_dict('records')[0].items() if count > 1]\n",
    "if column_freq:\n",
    "    print(f\"Columns that appeared more than once: {column_freq}\")\n",
    "    merged_df = merged_df.select(*[F.col(col).alias(col) for col in set(merged_df.columns)])\n",
    "\n",
    "print(\"Columns in the processed dataframe:\")\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6c96891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "output_folder = \"TempOutputFolder\"\n",
    "output_file_path = os.path.join(output_folder, \"part-00000-*.csv\")\n",
    "\n",
    "merged_df.coalesce(1).write.csv(output_folder, mode=\"overwrite\", header=True)\n",
    "\n",
    "for filename in os.listdir(output_folder):\n",
    "    if filename.startswith(\"part-00000\"):\n",
    "        shutil.move(os.path.join(output_folder, filename), \"Processed_Merged_Dataset.csv\")\n",
    "\n",
    "shutil.rmtree(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e0d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
